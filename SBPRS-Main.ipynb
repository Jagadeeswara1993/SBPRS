{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string,re\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for text preprocessing\n",
    "import  nltk, spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable = ['ner', 'parser'])\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\",None)\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "pd.set_option(\"display.width\",None)\n",
    "pd.set_option(\"display.max_colwidth\",None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"sample30.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   We observe that there are three columns that will be useful in building a sentiment classification model:\n",
    "### 1. `reviews_text`: It contains the reviews given by the users to a particular product\n",
    "### 2. `reviews_title`: It contains the title of the review given in previous column\n",
    "### 3. `user_sentiment`: It contains the overall sentiment of the user for a particular product (Positive or Negative). We will use them as labels in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 190 missing values in `reviews_title` ->  we will replace them with blank.\n",
    "###  There is 1 missing value in `user_sentiment`-> we will simply drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"reviews_title\"].fillna(\"\",inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[~df[\"user_sentiment\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   We now create a new dataframe containing only two columns:-\n",
    "### 1. First column will be a concatenation of the two columns: `reviews_text` and `reviews_title`.\n",
    "### 2. Second column will be the `user_sentiment` column and it will serve as our target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviews']=df['reviews_text']+\" \"+df[\"reviews_title\"]\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=df[[\"reviews\",\"user_sentiment\"]]\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"user_sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.countplot(x=\"user_sentiment\",data=final_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  As our target column is highly imbalanced, we will have to use techniques to handle imbalanced data during our model building process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ` Text Preprocessing `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    # Make the text lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"reviews\"]=final_df[\"reviews\"].apply(lambda x:clean_text(x))\n",
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    text = nlp(text)\n",
    "    text = [token.lemma_ for token in text]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"reviews\"]=final_df[\"reviews\"].apply(lambda x:lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"reviews\"] = final_df[\"reviews\"].str.replace('-PRON-','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lens = [len(d) for d in final_df['reviews']]\n",
    "doc_lens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(doc_lens, edgecolor='black', bins = 50)\n",
    "plt.title('Distribution of Review character length', fontsize=25)\n",
    "plt.ylabel('Number of Reviews', fontsize=20)\n",
    "plt.xlabel('Review character length', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color='white',\n",
    "                      stopwords=stop_words,\n",
    "                      max_words=40,\n",
    "                      max_font_size=40,\n",
    "                      scale=30,\n",
    "                      random_state=42\n",
    "                      ).generate(str(final_df[\"reviews\"]))\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_df, open('picklecopy\\processed_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Feature Extraction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=final_df[\"reviews\"]\n",
    "y=final_df[\"user_sentiment\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(ngram_range = (1,3),stop_words='english',max_df=.95,min_df=2)\n",
    "X_tfv=tfv.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv_feature_names=tfv.get_feature_names()\n",
    "len(tfv_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tfv.vocabulary_, open(\"picklecopy/tfidf_vocabulary.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfv, X_test_tfv, y_train, y_test = train_test_split(X_tfv, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfv.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfv.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Model Building`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `1.Logistic Regression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit=LogisticRegression()\n",
    "logit.fit(X_train_tfv,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train=logit.predict(X_train_tfv)\n",
    "print(accuracy_score(y_pred_train,y_train))\n",
    "print(classification_report(y_pred_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train,y_pred_train)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test=logit.predict(X_test_tfv)\n",
    "print(accuracy_score(y_pred_test,y_test))\n",
    "print(classification_report(y_pred_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the Precision is very low for negative sentiment and the model is biased towards the majority case.We can increase the f1 score by `SMOTE` technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm=SMOTE(random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfvsm,y_trainsm=sm.fit_resample(X_train_tfv,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_trainsm.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y_train)\n",
    "print('Before',counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y_trainsm)\n",
    "print('After',counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logitsm=LogisticRegression()\n",
    "logitsm.fit(X_train_tfvsm,y_trainsm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_trainsm=logitsm.predict(X_train_tfvsm)\n",
    "print(accuracy_score(y_pred_trainsm,y_trainsm))\n",
    "print(classification_report(y_pred_trainsm,y_trainsm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_trainsm,y_pred_trainsm)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,y_pred_testsm)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_testsm=logitsm.predict(X_test_tfv)\n",
    "print(accuracy_score(y_pred_testsm,y_test))\n",
    "print(classification_report(y_pred_testsm,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_lr = f1_score(y_test, y_pred_testsm, average=\"weighted\")\n",
    "f1_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning to improve performance of the model:\n",
    "param_grid_lr = {\n",
    "     'penalty': ['l1', 'l2'],\n",
    "     'C': [0.001,0.01,0.1,1,10,100],\n",
    "     'solver':['liblinear', 'saga']\n",
    "    }\n",
    "\n",
    "grid_lr = GridSearchCV(estimator=logitsm, \n",
    "                       param_grid=param_grid_lr,\n",
    "                       verbose=1,\n",
    "                       scoring='roc_auc',\n",
    "                       n_jobs=-1,\n",
    "                       cv=4)\n",
    "grid_lr.fit(X_train_tfvsm, y_trainsm)\n",
    "print(grid_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logitsm_tune=LogisticRegression(C=100, \n",
    "                               penalty='l2', \n",
    "                               solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logitsm_tune.fit(X_train_tfvsm, y_trainsm)\n",
    "y_pred_testsm_tune = logitsm_tune.predict(X_test_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,y_pred_testsm_tune)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_pred_testsm_tune,y_test))\n",
    "print(classification_report(y_pred_testsm_tune,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_lr_tune = f1_score(y_test, y_pred_testsm_tune, average=\"weighted\")\n",
    "f1_lr_tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.Random Forest` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc=RandomForestClassifier(n_estimators=50)\n",
    "rfc.fit(X_train_tfvsm,y_trainsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfc=rfc.predict(X_test_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,y_pred_rfc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_rfc = f1_score(y_test, y_pred_rfc, average=\"weighted\")\n",
    "f1_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_pred_rfc,y_test))\n",
    "print(classification_report(y_pred_rfc,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning to improve performance of the model:\n",
    "param_grid_rf = {\n",
    "     'n_estimators': [50,100,150],\n",
    "     'criterion':['gini','entropy'],\n",
    "     'max_depth': [30,40,50],\n",
    "     'min_samples_split': [2, 5, 10],\n",
    "     'min_samples_leaf': [1, 5, 10],\n",
    "       }\n",
    "\n",
    "grid_rfc = GridSearchCV(estimator=rfc, \n",
    "                        param_grid=param_grid_rf,\n",
    "                        scoring='roc_auc',\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        cv=3)\n",
    "grid_rfc.fit(X_train_tfvsm, y_trainsm)\n",
    "print(grid_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_tune=RandomForestClassifier(criterion= 'gini', \n",
    "                                max_depth= 50, \n",
    "                                min_samples_leaf= 1, \n",
    "                                min_samples_split= 2, \n",
    "                                n_estimators= 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_tune.fit(X_train_tfvsm,y_trainsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfctune=rfc_tune.predict(X_test_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,y_pred_rfctune)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_rfc_tune = f1_score(y_test, y_pred_rfctune, average=\"weighted\")\n",
    "f1_rfc_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_pred_rfctune,y_test))\n",
    "print(classification_report(y_pred_rfctune,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `3.Naive Bayes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfvsm,y_trainsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nb=nb.predict(X_test_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,y_pred_nb)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_nb = f1_score(y_test, y_pred_nb, average=\"weighted\")\n",
    "f1_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_pred_nb,y_test))\n",
    "print(classification_report(y_pred_nb,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning to improve performance of the model:\n",
    "param_grid_nb = {\n",
    "     'alpha': (1, 0.1, 0.01, 0.001, 0.0001, 0.00001),\n",
    "     'fit_prior':[True, False]\n",
    " }\n",
    "grid_nb = GridSearchCV(estimator=nb, \n",
    "                       param_grid=param_grid_nb,\n",
    "                       verbose=1,\n",
    "                       scoring='f1_weighted',\n",
    "                       n_jobs=-1,\n",
    "                       cv=10)\n",
    "grid_nb.fit(X_train_tfvsm, y_trainsm)\n",
    "print(grid_nb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tune = MultinomialNB(alpha=0.00001, fit_prior=True)\n",
    "nb_tune.fit(X_train_tfvsm,y_trainsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nbtune=nb_tune.predict(X_test_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,y_pred_nbtune)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_nb_tune = f1_score(y_test, y_pred_nbtune, average=\"weighted\")\n",
    "f1_nb_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_pred_nbtune,y_test))\n",
    "print(classification_report(y_pred_nbtune,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[[\"Logistic Regression\",round(f1_lr,2),round(f1_lr_tune,2)],\n",
    "      [\"Random Forest\",round(f1_rfc,2),round(f1_rfc_tune,2)],\n",
    "      [\"Naive Bayes\",round(f1_nb,2),round(f1_nb_tune,2)]]\n",
    "summary=pd.DataFrame(data,columns=[\"Model Name\",\"F1_score(untune)\",\"F1_score(tune)\"])\n",
    "summary\n",
    "                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model XGBOOST takes more time than other models and it size is more and hence it cannot be deployed in `HEROKU` (cloud application platform). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  From the above summary table, we compare all the models built and select the `tuned Logistic Regression` model. There are two reasons we select this model:-\n",
    "\n",
    "### 1. This model gives the highest `weighted F1 Score` \n",
    "\n",
    "### 2. It takes much less time to train than other models giving same `weighted F1 Score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(logitsm_tune, open(\"picklecopy/Tuned_logreg_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building a recommendation system "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will perform the following sub-tasks:\n",
    "\n",
    "2.1 - **User-based Recommendation System**  \n",
    "2.2 - **Item-based Recommendation System**  \n",
    "2.3 - **Select best Recommendation System**  \n",
    "2.4 - **Recommend top-20 products to user**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 User-based Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings=df[[\"reviews_username\",\"name\",\"reviews_rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the rows in reviews_username\n",
    "ratings=ratings[~ratings[\"reviews_username\"].isnull()]\n",
    "ratings.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.drop_duplicates(subset=[\"reviews_username\",\"name\",\"reviews_rating\"],keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and Train split of the dataset.\n",
    "train,test=train_test_split(ratings,test_size=0.3,random_state=42)\n",
    "print(\"shape of the train set = {}\".format(train.shape))\n",
    "print(\"shape of the test set = {}\".format(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pivot table with all user names as index, all products as columns and ratings as values\n",
    "# Note: Here we use fillna(0) to give 0 ratings to products that have not been rated by corresponding user\n",
    "df_pivot = train.pivot_table(\n",
    "    index=\"reviews_username\",\n",
    "    columns='name',\n",
    "    values='reviews_rating'\n",
    ").fillna(0)\n",
    "\n",
    "# View first five rows of the pivot table\n",
    "df_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `dummy train` will be used for prediction of ratings given by peer users to the products that have not been rated by user `u`. For this, we create a copy of the training dataset and then give `0` rating to the products that have already been rated and `1` to the non-rated products.\n",
    "####  `dummy test` will be used for evaluation. As we want to evaluate only those products that have been rated, we give `1` rating to the products that have already been rated by user `u` and `0` to the non-rated products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy the train dataset into dummy_train.\n",
    "dummy_train=train.copy()\n",
    "dummy_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_train.reviews_rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The products not rated by user is marked as 1 for prediction.\n",
    "\n",
    "dummy_train[\"reviews_rating\"]=dummy_train[\"reviews_rating\"].apply(lambda x:0 if x>=1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dummy train dataset into matrix format.\n",
    "dummy_train=dummy_train.pivot_table(\n",
    "        index=\"reviews_username\",\n",
    "        columns=\"name\",\n",
    "        values=\"reviews_rating\",\n",
    "        ).fillna(1)\n",
    "\n",
    "dummy_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we find similarity between the users by using ` cosine similarity` metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the user similarity Matrix using pairwise_distance function\n",
    "user_correlation = 1 - pairwise_distances(df_pivot, metric='cosine')\n",
    "user_correlation[np.isnan(user_correlation)] = 0\n",
    "print(user_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_correlation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction (UBCF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace negative correlations with 0\n",
    "user_correlation[user_correlation<0]=0\n",
    "user_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication of matrices user similarity matrix and original matrix df_pivot\n",
    "# This will give predicted ratings of the users corresponding to each product in the dataset\n",
    "user_predicted_ratings = np.dot(user_correlation, df_pivot.fillna(0))\n",
    "user_predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_predicted_ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  We are only interested in products that are not rated by user `u`. So we ignore the rated products by setting their ratings as `0`.\n",
    "###   For this, we multiply the `dummy_train` matrix with the `user_predicted_ratings` matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplying 'dummy_train' with 'user_predicted_ratings' to make ratings of non-rated products 0\n",
    "user_final_rating = np.multiply(user_predicted_ratings, dummy_train)\n",
    "user_final_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_final_rating.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = user_final_rating.loc['lucky'].sort_values(ascending=False)[0:20]\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evaluation (UBCF) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation is the same as prediction except for one thing: here we will evaluate for products that are already rated by user `u` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common = test[test.reviews_username.isin(train.reviews_username)]\n",
    "common.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into the user-product matrix (pivot form)\n",
    "common_user_based_matrix = common.pivot_table(index='reviews_username', columns='name', values='reviews_rating')\n",
    "common_user_based_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_user_based_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we will filter out correlations of those users that are common in both train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the user_correlation matrix into dataframe.\n",
    "user_correlation_df = pd.DataFrame(user_correlation)\n",
    "user_correlation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index of user correlation df as index of df_pivot\n",
    "user_correlation_df['reviews_username'] = df_pivot.index\n",
    "user_correlation_df.set_index('reviews_username',inplace=True)\n",
    "user_correlation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all user names in a list\n",
    "list_name = common.reviews_username.tolist()\n",
    "\n",
    "# Set column names of user correlation df as indices of df_subtracted\n",
    "user_correlation_df.columns = df_pivot.index.tolist()\n",
    "\n",
    "# Filter out those user correlations that are present in both\n",
    "user_correlation_df_1 =  user_correlation_df[user_correlation_df.index.isin(list_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_correlation_df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_correlation_df_2 = user_correlation_df_1.T[user_correlation_df_1.T.index.isin(list_name)]\n",
    "\n",
    "user_correlation_df_3 = user_correlation_df_2.T\n",
    "\n",
    "user_correlation_df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_correlation_df_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set negative correlations to 0\n",
    "user_correlation_df_3[user_correlation_df_3<0]=0\n",
    "\n",
    "# Dot product of 'user_correlation_df_3' and 'common_user_based_matrix'\n",
    "common_user_predicted_ratings = np.dot(user_correlation_df_3, common_user_based_matrix.fillna(0))\n",
    "common_user_predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the predicted ratings of users that have already rated the products\n",
    "dummy_test = common.copy()\n",
    "\n",
    "dummy_test['reviews_rating'] = dummy_test[\"reviews_rating\"].apply(lambda x: 1 if x>=1 else 0)\n",
    "\n",
    "dummy_test = dummy_test.pivot_table(index='reviews_username', columns='name', values='reviews_rating').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply 'common_user_predicted_ratings' with 'dummy_test'\n",
    "common_user_predicted_ratings = np.multiply(common_user_predicted_ratings,dummy_test)\n",
    "\n",
    "# Check first few rows\n",
    "common_user_predicted_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Now, we have to calculate the RMSE for only the products rated by the users\n",
    "###  For this, we normalize the ratings to bring them in range 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "# Create a copy of 'common_user_predicted_ratings'\n",
    "X  = common_user_predicted_ratings.copy() \n",
    "\n",
    "# Filter out positive ratings\n",
    "X = X[X>0]\n",
    "\n",
    "# Normalize the ratings and bring them within range 1 to 5\n",
    "scaler = MinMaxScaler(feature_range=(1, 5))\n",
    "print(scaler.fit(X))\n",
    "y = (scaler.transform(X))\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_non_nan = np.count_nonzero(~np.isnan(y))\n",
    "total_non_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = (sum(sum((common_user_based_matrix - y )**2))/total_non_nan)**0.5\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Item-based Recommendation System "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table with all user names as index, all products as columns and ratings as values\n",
    "# Note: Here we take a transpose to get products (items) as indices and users as columns\n",
    "df_pivot = train.pivot_table(\n",
    "    index='reviews_username',\n",
    "    columns='name',\n",
    "    values='reviews_rating'\n",
    ").T.fillna(0)\n",
    "\n",
    "# View first five rows of the pivot table\n",
    "df_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the item similarity Matrix using pairwise_distance function\n",
    "item_correlation = 1 - pairwise_distances(df_pivot, metric='cosine')\n",
    "item_correlation[np.isnan(item_correlation)] = 0\n",
    "print(item_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace negative correlations with 0\n",
    "item_correlation[item_correlation<0]=0\n",
    "item_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction - Item Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication of matrices user similarity matrix and transposed matrix df_pivot\n",
    "# This will give predicted ratings of the users corresponding to each product in the dataset\n",
    "item_predicted_ratings = np.dot((df_pivot.fillna(0).T),item_correlation)\n",
    "item_predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_predicted_ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the above shape is same as that of 'dummy_train'\n",
    "dummy_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplying 'dummy_train' with 'item_predicted_ratings' to make ratings of rated products 0\n",
    "item_final_rating = np.multiply(item_predicted_ratings,dummy_train)\n",
    "item_final_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_final_rating.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommending top 20 products to the user.\n",
    "d = item_final_rating.loc['00sab00'].sort_values(ascending=False)[0:20]\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (IBCF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract users from test dataset that are present in the train dataset\n",
    "common = test[test['name'].isin(train['name'])]\n",
    "common.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_item_based_matrix = common.pivot_table(index='reviews_username', columns='name', values='reviews_rating').T\n",
    "common_item_based_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "common_item_based_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the item_correlation matrix into dataframe.\n",
    "item_correlation_df = pd.DataFrame(item_correlation)\n",
    "item_correlation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index of item correlation df as index of df_pivot\n",
    "item_correlation_df['name'] = df_pivot.index\n",
    "item_correlation_df.set_index('name',inplace=True)\n",
    "item_correlation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all product names in a list\n",
    "list_name = common['name'].tolist()\n",
    "\n",
    "# Set column names of item correlation df as indices of df_subtracted\n",
    "item_correlation_df.columns = df_pivot.index.tolist()\n",
    "\n",
    "# Filter out those item correlations that are present in both train and test datasets\n",
    "item_correlation_df_1 =  item_correlation_df[item_correlation_df.index.isin(list_name)]\n",
    "\n",
    "item_correlation_df_2 = item_correlation_df_1.T[item_correlation_df_1.T.index.isin(list_name)]\n",
    "\n",
    "item_correlation_df_3 = item_correlation_df_2.T\n",
    "\n",
    "item_correlation_df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set negative correlations to 0\n",
    "item_correlation_df_3[item_correlation_df_3<0]=0\n",
    "\n",
    "# Dot product of 'item_correlation_df_3' and 'common_item_based_matrix'\n",
    "common_item_predicted_ratings = np.dot(item_correlation_df_3, common_item_based_matrix.fillna(0))\n",
    "common_item_predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the predicted ratings of products that have not been rated by the user\n",
    "dummy_test = common.copy()\n",
    "\n",
    "dummy_test['reviews_rating'] = dummy_test['reviews_rating'].apply(lambda x: 1 if x>=1 else 0)\n",
    "\n",
    "dummy_test = dummy_test.pivot_table(index='reviews_username', columns='name', values='reviews_rating').T.fillna(0)\n",
    "\n",
    "# Multiply 'common_item_predicted_ratings' with 'dummy_test'\n",
    "common_item_predicted_ratings = np.multiply(common_item_predicted_ratings,dummy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_item_predicted_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "# Create a copy of 'common_item_predicted_ratings'\n",
    "X  = common_item_predicted_ratings.copy() \n",
    "\n",
    "# Filter out positive ratings\n",
    "X = X[X>0]\n",
    "\n",
    "# Normalize the ratings and bring them within range 1 to 5\n",
    "scaler = MinMaxScaler(feature_range=(1, 5))\n",
    "print(scaler.fit(X))\n",
    "y = (scaler.transform(X))\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_non_nan = np.count_nonzero(~np.isnan(y))\n",
    "total_non_nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = (sum(sum((common_item_based_matrix - y )**2))/total_non_nan)**0.5\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-based correlation filter that we built gives an RMSE value of **3.57**.\n",
    "### RMSE of `IBCF` (**3.57**) is higher than RMSE of `UBCF` (**2.13**).\n",
    "### Thus, we will chose the `UBCF` as our recommendation system as it has less error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the final ratings in a pickle file\n",
    "pickle.dump(user_final_rating.astype('float32'), open('picklecopy/user_final_rating.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
